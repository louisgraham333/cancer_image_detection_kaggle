{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification Script.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisgraham333/cancer_image_detection_kaggle/blob/main/Image_Classification_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjEtUcv-fXYy"
      },
      "source": [
        "# Histopathologic Cancer Detection\n",
        "*This notebook is created for the Kaggle Histopathologic Cancer Detection Challenge. It pulls the data from Kaggle, analyses it, and creates a set of estimates for the validation set*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLPejbKpf5dL"
      },
      "source": [
        "## Chapters\n",
        "\n",
        "\n",
        "1. Prepare the script and pull data from Kaggle\n",
        "2. Download the data, and move to correct folders\n",
        "3. Create and fit the model (Keras-Tensorflow)\n",
        "4. Predict on the validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6cJ_xKbgglr"
      },
      "source": [
        "## Chapter 1: Prepare the script\n",
        "Import packages, set up the file directory, and set up the link to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMKVXpaegs0M"
      },
      "source": [
        "###Install and import packages\n",
        "!pip install kaggle\n",
        "!pip install -U -q kaggle\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import time\n",
        "import shutil\n",
        "import skimage.io as io\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow import set_random_seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfT01xCUhjBr"
      },
      "source": [
        "#Authenticate Google Drive (to allow saving)\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()                       \n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ5ahlghN8-m"
      },
      "source": [
        "#Set seed object\n",
        "seed_object = np.random.seed(7654)\n",
        "random.seed(seed_object)\n",
        "set_random_seed(101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIp0ly9B6x-K"
      },
      "source": [
        "#Create directory structure for image generator to work off\n",
        "base_dir = 'base_dir'\n",
        "os.mkdir(base_dir)\n",
        "train_all_dir = os.path.join(base_dir, 'train_all_dir')\n",
        "os.mkdir(train_all_dir)\n",
        "train_dir = os.path.join(base_dir, 'train_dir')\n",
        "os.mkdir(train_dir)\n",
        "no_tumor_tissue = os.path.join(train_dir, 'a_no_tumor_tissue')\n",
        "os.mkdir(no_tumor_tissue)\n",
        "has_tumor_tissue = os.path.join(train_dir, 'b_has_tumor_tissue')\n",
        "os.mkdir(has_tumor_tissue)\n",
        "val_dir = os.path.join(base_dir, 'val_dir')\n",
        "os.mkdir(val_dir)\n",
        "no_tumor_tissue = os.path.join(val_dir, 'a_no_tumor_tissue')\n",
        "os.mkdir(no_tumor_tissue)\n",
        "has_tumor_tissue = os.path.join(val_dir, 'b_has_tumor_tissue')\n",
        "os.mkdir(has_tumor_tissue)\n",
        "test_dir = os.path.join(base_dir, 'test_dir')\n",
        "os.mkdir(test_dir)\n",
        "test_images = os.path.join(test_dir, 'test_images')\n",
        "os.mkdir(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74mEspu3hVlX"
      },
      "source": [
        "###Set up connection to Kaggle and upload API file\n",
        "!mkdir -p ~/.kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaJzx7-jhu2a"
      },
      "source": [
        "###Store API file, make private and view API\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions list   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hrL4fVPkqY2"
      },
      "source": [
        "###Listfiles in the competition\n",
        "!kaggle competitions files histopathologic-cancer-detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfmsDe1vI0vk"
      },
      "source": [
        "##Chapter 2: Download the data, and move to correct folders\n",
        "Pull the data from the Kaggle API to colab, and then shift into the correct folders for the data generator to work off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bf5wYciwj4_"
      },
      "source": [
        "###Download sample submission, unzip and delete zip\n",
        "os.chdir('/content/base_dir')\n",
        "!kaggle competitions download -f sample_submission.csv histopathologic-cancer-detection\n",
        "!unzip -q sample_submission.csv\n",
        "os.remove(\"sample_submission.csv.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at69JCxgxDFP"
      },
      "source": [
        "###Download train labels, unzip and delete zip\n",
        "!kaggle competitions download -f train_labels.csv histopathologic-cancer-detection\n",
        "!unzip -q train_labels.csv\n",
        "os.remove(\"train_labels.csv.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvxz4MN-HUDC"
      },
      "source": [
        "###Download train, unzip and delete zip\n",
        "os.chdir('/content/base_dir/train_all_dir')\n",
        "!kaggle competitions download -f train.zip histopathologic-cancer-detection\n",
        "!unzip -q train\n",
        "os.remove(\"train.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HWB2zdi8xWa"
      },
      "source": [
        "###Download test, unzip and delete zip\n",
        "os.chdir('/content/base_dir/test_dir/test_images')\n",
        "!kaggle competitions download -f test.zip histopathologic-cancer-detection\n",
        "!unzip -q test\n",
        "os.remove(\"test.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJpfVLZuHHCX"
      },
      "source": [
        "#Check number of objects in each\n",
        "print(sum([len(files) for r, d, files in os.walk(\"/content/base_dir/train_all_dir\")]))\n",
        "print(sum([len(files) for r, d, files in os.walk(\"/content/base_dir/test_dir\")]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHXjgtnFJTFe"
      },
      "source": [
        "#Open and inspect labels\n",
        "os.chdir('/content/base_dir')\n",
        "labels = pd.read_csv(\"train_labels.csv\")\n",
        "labels.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q_fvoItJbPG"
      },
      "source": [
        "#Split labels into train and validation\n",
        "msk = np.random.rand(len(labels)) < 0.9\n",
        "train_labels = labels[msk]\n",
        "val_labels = labels[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kwbBLiPJeGz"
      },
      "source": [
        "#Split labels into has tumor and no tumor\n",
        "train_has_labels = train_labels['id'].loc[train_labels['label'] == 1]\n",
        "train_no_labels = train_labels['id'].loc[train_labels['label'] == 0]\n",
        "val_has_labels = val_labels['id'].loc[val_labels['label'] == 1]\n",
        "val_no_labels = val_labels['id'].loc[val_labels['label'] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2Y8ESrUNjGi"
      },
      "source": [
        "#Shift train with labels to correct directory\n",
        "for i in train_has_labels:\n",
        "  shutil.copyfile(\"train_all_dir/\" + str(i) + \".tif\", \"train_dir/b_has_tumor_tissue/\" + str(i) + \".tif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyTfmqi8Nn0j"
      },
      "source": [
        "#Shift train without labels to correct directory\n",
        "for i in train_no_labels:\n",
        "  shutil.copyfile(\"train_all_dir/\" + str(i) + \".tif\", \"train_dir/a_no_tumor_tissue/\" + str(i) + \".tif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znZiu2a8JhGF"
      },
      "source": [
        "#Shift val with labels to correct directory\n",
        "for i in val_has_labels:\n",
        "  shutil.copyfile(\"train_all_dir/\" + str(i) + \".tif\", \"val_dir/b_has_tumor_tissue/\" + str(i) + \".tif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUtiHWFqNuP1"
      },
      "source": [
        "#Shift val without labels to correct directory\n",
        "for i in val_no_labels:\n",
        "  shutil.copyfile(\"train_all_dir/\" + str(i) + \".tif\", \"val_dir/a_no_tumor_tissue/\" + str(i) + \".tif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4daycvXQvoP"
      },
      "source": [
        "#Check numbers to make sure that they add up\n",
        "print(\"All train: \" + str(sum([len(files) for r, d, files in os.walk(\"/content/base_dir/train_all_dir\")])))\n",
        "print(\"Train no tumour: \" + str(sum([len(files) for r, d, files in os.walk(\"/content/base_dir/train_dir/a_no_tumor_tissue\")])))\n",
        "print(\"Train with tumour: \" + str(sum([len(files) for r, d, files in os.walk(\"/content/base_dir/train_dir/b_has_tumor_tissue\")])))\n",
        "print(\"Val no tumour: \" + str(sum([len(files) for r, d, files in os.walk(\"/content/base_dir/val_dir/a_no_tumor_tissue/\")])))\n",
        "print(\"Val with tumour: \" + str(sum([len(files) for r, d, files in os.walk(\"/content/base_dir/val_dir/b_has_tumor_tissue\")])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NzXY08TSKhx"
      },
      "source": [
        "##Chapter 3: Create and fit the model\n",
        "The model is created using Keras, and the inbuilt data generation functions (as not all data can be held in memory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrXCFpLWSZ-j"
      },
      "source": [
        "#Inspect images\n",
        "f, axarr = plt.subplots(2,4)\n",
        "axarr[0,0].imshow(io.imread(\"train_dir/b_has_tumor_tissue/\" + str(train_has_labels.iloc[random.randint(1,71429)])+\".tif\"))\n",
        "axarr[0,1].imshow(io.imread(\"train_dir/b_has_tumor_tissue/\" + str(train_has_labels.iloc[random.randint(1,71429)])+\".tif\"))\n",
        "axarr[0,2].imshow(io.imread(\"train_dir/b_has_tumor_tissue/\" + str(train_has_labels.iloc[random.randint(1,71429)])+\".tif\"))\n",
        "axarr[0,3].imshow(io.imread(\"train_dir/b_has_tumor_tissue/\" + str(train_has_labels.iloc[random.randint(1,71429)])+\".tif\"))\n",
        "axarr[1,0].imshow(io.imread(\"train_dir/a_no_tumor_tissue/\" + str(train_no_labels.iloc[random.randint(1,104722)])+\".tif\"))\n",
        "axarr[1,1].imshow(io.imread(\"train_dir/a_no_tumor_tissue/\" + str(train_no_labels.iloc[random.randint(1,104722)])+\".tif\"))\n",
        "axarr[1,2].imshow(io.imread(\"train_dir/a_no_tumor_tissue/\" + str(train_no_labels.iloc[random.randint(1,104722)])+\".tif\"))\n",
        "axarr[1,3].imshow(io.imread(\"train_dir/a_no_tumor_tissue/\" + str(train_no_labels.iloc[random.randint(1,104722)])+\".tif\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkXpF2VWQVJ8"
      },
      "source": [
        "#Variables needed for data generators\n",
        "train_path = \"train_dir\"\n",
        "val_path = \"val_dir\"\n",
        "image_size = 96\n",
        "train_batch_size = 32\n",
        "train_length = sum([len(files) for r, d, files in os.walk(\"/content/base_dir/train_dir/a_no_tumor_tissue\")]) + sum([len(files) for r, d, files in os.walk(\"/content/base_dir/train_dir/b_has_tumor_tissue\")])\n",
        "train_steps = np.ceil(train_length / train_batch_size)\n",
        "val_batch_size = 1024\n",
        "val_length = sum([len(files) for r, d, files in os.walk(\"/content/base_dir/val_dir/a_no_tumor_tissue\")]) + sum([len(files) for r, d, files in os.walk(\"/content/base_dir/val_dir/b_has_tumor_tissue\")])\n",
        "val_steps = np.ceil(val_length / val_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6v3DeftMhVY"
      },
      "source": [
        "#Set up the data generators\n",
        "os.chdir('/content/base_dir')\n",
        "datagen_train = ImageDataGenerator(rescale=1.0/255)\n",
        "datagen_val = ImageDataGenerator(rescale=1.0/255)\n",
        "train_gen = datagen_train.flow_from_directory(train_path,\n",
        "                                        target_size=(image_size,image_size),\n",
        "                                        batch_size=train_batch_size,\n",
        "                                        class_mode='categorical')\n",
        "val_gen = datagen_val.flow_from_directory(val_path,\n",
        "                                        target_size=(image_size,image_size),\n",
        "                                        batch_size=val_batch_size,\n",
        "                                        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrbuE8ooTCf3"
      },
      "source": [
        "#Set up the model (3 conv layers, then fully connected)\n",
        "kernel_size = (3,3)\n",
        "pool_size = (2,2)\n",
        "first_filters = 32\n",
        "second_filters = 64\n",
        "third_filters = 128\n",
        "dropout = 0.3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu',  input_shape = (96, 96, 3)))\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Conv2D(second_filters, kernel_size, activation = 'relu'))\n",
        "model.add(Conv2D(second_filters, kernel_size, activation = 'relu'))\n",
        "model.add(Conv2D(second_filters, kernel_size, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Conv2D(third_filters, kernel_size, activation = 'relu'))\n",
        "model.add(Conv2D(third_filters, kernel_size, activation = 'relu'))\n",
        "model.add(Conv2D(third_filters, kernel_size, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDhRrYv3Srw8"
      },
      "source": [
        "#Compile the model\n",
        "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l3sJY7NM08j"
      },
      "source": [
        "#Save files, and allow the learning rate to adjust if results plateau\n",
        "filepath = \"model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n",
        "                                   verbose=1, mode='max', min_lr=0.00001)\n",
        "callbacks_list = [checkpoint, reduce_lr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixxw8ye9ydAA"
      },
      "source": [
        "#Get IDs of the files in the folder\n",
        "file_list = drive.ListFile({'q': \"'1jplSxJOM7qGPlGjt0gw_WgQrN7QJQhq_' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPP1AXoQydMs"
      },
      "source": [
        "#Download models from Google Drive\n",
        "file_obj = drive.CreateFile({'id': '1ctcFJi_zmAArhtc5bBJpq6qJq_z15TSZ'})                       \n",
        "file_obj.GetContentFile('model_end.h5')\n",
        "file_obj = drive.CreateFile({'id': '13SZS14FnMm_VJAD4WF16P7FHSW7y_PHO'})                       \n",
        "file_obj.GetContentFile('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyCUc2Toy96A"
      },
      "source": [
        "#Load model\n",
        "os.chdir('/content/base_dir')\n",
        "model.load_weights(\"model_end.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLKYW3FmS6o0"
      },
      "source": [
        "#Train the model\n",
        "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=val_steps,\n",
        "                    epochs=5, verbose=1,\n",
        "                    callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b9sXM8GxuUt"
      },
      "source": [
        "#Save the final model\n",
        "model.save('model_end.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8njR5V64-xzG"
      },
      "source": [
        "#Refresh tokens\n",
        "gauth.Refresh()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpMoZ3i_g0Xv"
      },
      "source": [
        "#Save model to Google Drive (best, and current)\n",
        "model_file = drive.CreateFile({'title' : 'Base_Model_end_32_5.h5', \"parents\": [{\n",
        "  \"kind\": \"drive#childList\",\n",
        "  \"id\": \"\"}]})                       \n",
        "model_file.SetContentFile('model_end.h5')                       \n",
        "model_file.Upload()\n",
        "model_file = drive.CreateFile({'title' : 'Base_Model_best_32_5.h5', \"parents\": [{\n",
        "  \"kind\": \"drive#childList\",\n",
        "  \"id\": \"\"}]})                       \n",
        "model_file.SetContentFile('model.h5')                       \n",
        "model_file.Upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlU9EIX-G-Ms"
      },
      "source": [
        "##Section 4: Predict on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVT5hVGKHBKJ"
      },
      "source": [
        "#Pull best model and check accuracy\n",
        "os.chdir(\"/content/base_dir\")\n",
        "model.load_weights('model.h5')\n",
        "val_loss, val_acc = model.evaluate_generator(val_gen, steps=val_length)\n",
        "print('val_loss:', val_loss)\n",
        "print('val_acc:', val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrABTA4p8OFu"
      },
      "source": [
        "#Set up generator for the test data\n",
        "test_path =\"test_dir\"\n",
        "datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "test_gen = datagen.flow_from_directory(test_path,\n",
        "                                        target_size=(image_size,image_size),\n",
        "                                        batch_size=1024,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_ARLrcCHViy"
      },
      "source": [
        "#Make predictions\n",
        "num_test_images = 57458\n",
        "predictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)\n",
        "len(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNdr9PLfHlea"
      },
      "source": [
        "#Clean \n",
        "df_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n",
        "test_filenames = test_gen.filenames\n",
        "df_preds['file_names'] = test_filenames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr_jxYo6Kx6b"
      },
      "source": [
        "#Add the ID column\n",
        "def extract_id(x):\n",
        "    # split into a list\n",
        "    a = x.split('/')\n",
        "    # split into a list\n",
        "    b = a[1].split('.')\n",
        "    extracted_id = b[0]\n",
        "    return extracted_id\n",
        "df_preds['id'] = df_preds['file_names'].apply(extract_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBp_bIqjLGmP"
      },
      "source": [
        "#Create final file\n",
        "y_pred = df_preds['has_tumor_tissue']\n",
        "image_id = df_preds['id']\n",
        "submission = pd.DataFrame({'id':image_id, \n",
        "                           'label':y_pred, \n",
        "                          }).set_index('id')\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9RxAba3LW2c"
      },
      "source": [
        "#Export predictions\n",
        "os.chdir(\"/content/base_dir\")\n",
        "submission.to_csv('preds.csv', columns=['label'])\n",
        "files.download('preds.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBClhvvqtdeH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}